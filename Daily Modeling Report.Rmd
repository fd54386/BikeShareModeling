---
title: "Daily Modeling Report"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r setup packages}
library(tidyverse)
library(caret)  
library(knitr)
# Day of week decoder
dfDOW<- data.frame(num<- 0:6, name <- c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'))
```
# Introduction 

We are working with a bikeshare dataset.  The number of bike renters using a Washington, D.C. based rental service on a daily basis from 2011-2012 was aggregated by researchers and made [available](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).  There are 16 variables in the daily dataset.  
* 1 Meta column, indicating the index of the observation in the dataset.  
* 7 date related columns -- the date itself, as well as helper variables indicating if it was a workday / holiday / etc.
* 5 weather related columns -- a grade for precipitation, as well as fields for temperature, heat index, humidity, and windspeed  
* 3 response variables -- A count of casual users, registered users, and the total count  

This report is focused on predicting the total user count on any `r dfDOW[1, 2]` given the date and weather information.  

We will perform this modeling using both a simple decision tree with leave-one-out cross validation, as well as a gradient boosted tree model with 5-fold cross validation.

# Data  

For linear modeling we would typically convert most of the discrete derived date variables as a factor to ensure that something like the cyclical nature of the seasons are treated more appropriately.  A decision tree with a sufficient number of levels should be able to reasonably account for nonlinear behavior, so we will not factorize those variables for this modeling.
```{r Import and Split Data}
#Import the data
tibDaily<- read_csv('day.csv')

tibDOW<- tibDaily%>% filter(weekday == '1')%>% select(-casual, -registered)

set.seed(1)
train <-sample(1:nrow(tibDOW), size =nrow(tibDOW)*0.7)
test <- dplyr::setdiff(1:nrow(tibDOW), train)
tibTrain <- tibDOW[train, ]
tibTest <- tibDOW[test, ]
```

# Training Data Summary 

This is a quick overview of the training dataset for this `r dfDOW[1, 2]` report.  It will be good to look at the number of holidays and working days in this set.  We will also plot the ridership trends vs time, and the relationship with the weather variables.  Note that the data has already been standardized, so we will not rescale it further.

```{r Exploratory Plots, warning = FALSE}
#Summarize holiday performance
tibSumm <- tibTrain %>% group_by(holiday)%>% 
  summarize(count = n(), avgRidership = mean(cnt, na.rm = TRUE))

#Plot out trends vs date
ggplot(data = tibTrain, mapping = aes(x = dteday, y = cnt, color = as.factor(season),
  shape = as.factor(workingday))) + geom_point()+
  ggtitle('Ridership vs Date') + xlab('Date') + ylab('Rider Count')

#Gather all continuous weather variables for easy faceting
tibWeather <- gather(tibTrain, key = "Variable", value = "value", -cnt)%>% 
  filter(Variable %in% c('weathersit', 'temp', 'atemp', 'hum', 'windspeed'))

#Scatterplots for weather
ggplot (data = tibWeather, mapping = aes(x = value, y = cnt)) + geom_point() +
  geom_density2d() +  facet_wrap(~Variable, scales = 'free') + xlab('Standardized Weather Metric') + 
  ylab('Rider Count') + ggtitle('Ridership vs Weather Data')
```  

# Model Fitting  

## Decision Tree  
We will start by fitting a simple decision tree using the caret package.   

We will omit the following variables from the predictors:  
* instant -- the data index, doesn't contain any real information.  
* dteday -- potentially confounds the other date variables.  
* weekday -- This is dependent on the day of week of the report, and will be a constant for every observation.
* workingday -- Since this report is based on a single day of the week, this field is the exact opposite of the holiday variable.  Excluding to simplify runtime.  

We will also add in the following interaction terms since they'd likely influence someone's desire to spend time outside of cover:  
* temp:holiday 
* windspeed:weathersit
* hum:temp

No quadratic terms will be used, we will rely on the decision tree algorithm to navigate non-linearities.

### Cross Validation  
We are running leave one out cross validation  
```{r CV setup}
#Define caret cv control information
trctrl <- trainControl(method = "LOOCV")
```

### Fit the Decision Tree  
We are running the `rpart` method in the `caret` package.  We will use the default tuning algorithm which should be more robust than simple checks for absolute RMSE minima in an automated report.
```{r Classification Tree}
#  Define a seed so each model starts at the same place 
set.seed(3333)
# Run the model with centered & scaled data, default tuning parameters
treeFit <- train(cnt ~ .-instant-dteday-weekday-workingday+
                 temp:holiday+windspeed:weathersit+hum:temp,
                 data = tibTrain, method = "rpart",
                 trControl=trctrl)

treeFit

plot(treeFit)
```  
## Gradient Boosted Tree  

We will omit the following variables from the predictors:  
* instant -- the data index, doesn't contain any real information.  
* dteday -- potentially confounds the other date variables.  
* weekday -- This is dependent on the day of week of the report, and will be a constant for every observation.
* workingday -- Since this report is based on a single day of the week, this field is the exact opposite of the holiday variable.  Excluding to simplify runtime.  

We will not explicitly call out any interaction terms since the gradient boosting algorithm includes a review of the interaction extent by default.  

### Cross Validation  
We will use 5-fold repeated cv for cross validating the boosted fit.  This should be faster than the leave-one-out fit we used on the previous model.  
```{r Boosted CV}
cvInfo <- trainControl(method = 'repeatedcv', number = 5, repeats = 3)
```

### Fit the Gradient Boosted Tree  
We are running the `gbm` method in the `caret` package.  We will use the default tuning algorithm as it should be more robust than a generic grid search in an automated report.  
```{r Boosted Tree}
set.seed(3333)
# Run the model with centered & scaled data, default tuning grid
boostFit <- train(class ~ ., data = tibTrain, method = "gbm",
 trControl=cvInfo)
```

